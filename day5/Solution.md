# Day 5

## Solution
Robots exclusion protocol is used by a website in order to notify web crawlers (like search engines) which parts of the site are not allowed to be visited or crawled.

The parts of a website which an admin doesn't want other people to visit often contain important information. So it's usually clever to take a look what's in `robots.txt`.

In this problem, you can find a "top secret" in `robots.txt` under WeChall's root directory. The "top secret" contains the flag.
